apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "my-inference-service-4"
  labels:
    app: nginx
spec:
  predictor:
     containers:
      - image: "docker.io/rahulyerme1234/aiml-model:1"
        ports:
          - containerPort: 5000
        #resources:
        #requests:
        #cpu: "0.5"
        # memory: "512Mi"
            #minReplicas: 1
            #maxReplicas: 1
